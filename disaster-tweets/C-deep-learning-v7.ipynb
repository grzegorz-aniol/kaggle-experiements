{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import base, feature_extraction, ensemble, model_selection, pipeline, compose, preprocessing, metrics\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "import tensorflow as tf\n",
    "from embedding_transformer import Doc2VecTransformer\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import optuna\n",
    "import pprint\n",
    "\n",
    "SCRIPT_NAME='DL-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6851, 25), (762, 25))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.read_csv('./train_enriched.csv', index_col='id')\n",
    "df_train, df_validation = model_selection.train_test_split(df_dataset, test_size=0.1)\n",
    "df_train.shape, df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>positive_factor</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>missing_location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_content</th>\n",
       "      <th>...</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>stop_words_factor</th>\n",
       "      <th>clean_tokens_factor</th>\n",
       "      <th>url_domains</th>\n",
       "      <th>url_redirects_count</th>\n",
       "      <th>hashtags_sentiment</th>\n",
       "      <th>token_sentiment</th>\n",
       "      <th>token_sentiment_2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>casualties</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Another day has passed and THANKFULLY Central ...</td>\n",
       "      <td>another day has passed and thankfully central ...</td>\n",
       "      <td>Another day has passed and THANKFULLY Central ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.945400</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>rescuers</td>\n",
       "      <td>0.914286</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Fears over missing migrants in Med: Rescuers s...</td>\n",
       "      <td>fears over missing migrants in med rescuers se...</td>\n",
       "      <td>Fears over missing migrants in Med: Rescuers s...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>abouthub.info</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.730616</td>\n",
       "      <td>0.540590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4968</th>\n",
       "      <td>meltdown</td>\n",
       "      <td>0.151515</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Currently: Uncontrollable meltdown number 2</td>\n",
       "      <td>currently uncontrollable meltdown number</td>\n",
       "      <td>Currently: Uncontrollable meltdown number 2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.117501</td>\n",
       "      <td>-0.279375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>blaze</td>\n",
       "      <td>0.131579</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>looks like a year of writing and computers is ...</td>\n",
       "      <td>looks like a year of writing and computers is ...</td>\n",
       "      <td>looks like a year of writing and computers is ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.728852</td>\n",
       "      <td>-0.172885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>tragedy</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>India</td>\n",
       "      <td>India</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Rly tragedy in MP: Some live to recount horror...</td>\n",
       "      <td>rly tragedy in mp some live to recount horror ...</td>\n",
       "      <td>Rly tragedy in MP: Some live to recount horror...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.730769</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.879189</td>\n",
       "      <td>0.303046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         keyword  positive_factor location        country state city  \\\n",
       "id                                                                     \n",
       "1416  casualties         0.800000      USA  United States              \n",
       "5736    rescuers         0.914286                                      \n",
       "4968    meltdown         0.151515                                      \n",
       "662        blaze         0.131579                                      \n",
       "6805     tragedy         0.611111    India          India              \n",
       "\n",
       "      missing_location                                               text  \\\n",
       "id                                                                          \n",
       "1416                 0  Another day has passed and THANKFULLY Central ...   \n",
       "5736                 1  Fears over missing migrants in Med: Rescuers s...   \n",
       "4968                 1        Currently: Uncontrollable meltdown number 2   \n",
       "662                  1  looks like a year of writing and computers is ...   \n",
       "6805                 0  Rly tragedy in MP: Some live to recount horror...   \n",
       "\n",
       "                                             clean_text  \\\n",
       "id                                                        \n",
       "1416  another day has passed and thankfully central ...   \n",
       "5736  fears over missing migrants in med rescuers se...   \n",
       "4968           currently uncontrollable meltdown number   \n",
       "662   looks like a year of writing and computers is ...   \n",
       "6805  rly tragedy in mp some live to recount horror ...   \n",
       "\n",
       "                                           text_content  ...  urls_count  \\\n",
       "id                                                       ...               \n",
       "1416  Another day has passed and THANKFULLY Central ...  ...           1   \n",
       "5736  Fears over missing migrants in Med: Rescuers s...  ...           1   \n",
       "4968        Currently: Uncontrollable meltdown number 2  ...           0   \n",
       "662   looks like a year of writing and computers is ...  ...           1   \n",
       "6805  Rly tragedy in MP: Some live to recount horror...  ...           0   \n",
       "\n",
       "      tokens_count  stop_words_factor  clean_tokens_factor    url_domains  \\\n",
       "id                                                                          \n",
       "1416            19           0.263158             0.684211   facebook.com   \n",
       "5736            20           0.350000             0.600000  abouthub.info   \n",
       "4968             5           0.000000             1.000000                  \n",
       "662             11           0.363636             0.545455    twitter.com   \n",
       "6805            26           0.269231             0.730769                  \n",
       "\n",
       "      url_redirects_count  hashtags_sentiment  token_sentiment  \\\n",
       "id                                                               \n",
       "1416                    2                 0.0         4.945400   \n",
       "5736                    2                 0.0         9.730616   \n",
       "4968                    0                 0.0        -1.117501   \n",
       "662                     1                 0.0        -1.728852   \n",
       "6805                    0                 0.0         7.879189   \n",
       "\n",
       "      token_sentiment_2 target  \n",
       "id                              \n",
       "1416           0.274744      1  \n",
       "5736           0.540590      1  \n",
       "4968          -0.279375      0  \n",
       "662           -0.172885      0  \n",
       "6805           0.303046      1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': '', 'url_domains': '', 'clean_text': ''}, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (6851, 169)\n",
      "X_train type=<class 'numpy.ndarray'>, shape=(6851, 169)\n",
      "Y_train shape=(6851,)\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'country',\n",
    "    'state',\n",
    "]\n",
    "numerical_features = [\n",
    "    'text_length', \n",
    "    'positive_factor',\n",
    "    'token_sentiment_2'\n",
    "]\n",
    "\n",
    "column_transformer = compose.ColumnTransformer(transformers=[\n",
    "    ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('numerical', preprocessing.StandardScaler(), numerical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "transformer = pipeline.Pipeline([\n",
    "    ('columns', column_transformer)\n",
    "])\n",
    "\n",
    "transformer.fit(df_train)\n",
    "X_train = transformer.transform(df_train)\n",
    "print('X_train shape', X_train.shape)\n",
    "\n",
    "Y_train = df_train['target']\n",
    "\n",
    "print(f'X_train type={type(X_train)}, shape={X_train.shape}')\n",
    "print(f'Y_train shape={Y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_validation shape (762, 169)\n",
      "Y_validation shape (762,)\n"
     ]
    }
   ],
   "source": [
    "X_validation = transformer.transform(df_validation)\n",
    "print('X_validation shape', X_validation.shape)\n",
    "\n",
    "Y_validation = df_validation['target']\n",
    "print('Y_validation shape', Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X_train.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "NN_SHAPE = [INPUT_SIZE, 32, 32, 16, 1]\n",
    "BATCH_SIZE= X_train.shape[0]\n",
    "MAX_EPOCHS = 40\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return tf.cast(tf.greater(tf.nn.sigmoid(x), .5), tf.int32)\n",
    "\n",
    "\n",
    "def build_model(layer_dims, use_dropout=False, dropout_rate_1=0.3, dropout_rate_2=0.1,\n",
    "                learning_rate=1e-3, \n",
    "                use_emma=False, emma_momentum=0.99, \n",
    "                use_regularizer=False, regularizer=0.01,\n",
    "                initializer='glorot_normal',\n",
    "                activation='relu'\n",
    "                ):\n",
    "    n_layers = len(layer_dims)\n",
    "    layers = []\n",
    "\n",
    "    for l in range(1, n_layers-1):\n",
    "        layer_kws = {}\n",
    "\n",
    "        if use_regularizer:\n",
    "            layer_kws['kernel_regularizer'] = tf.keras.regularizers.l2(regularizer)\n",
    "        if initializer:\n",
    "            layer_kws['kernel_initializer'] = initializer\n",
    "        \n",
    "        hidden_layer= tf.keras.layers.Dense(layer_dims[l], input_shape=(layer_dims[l-1],), activation=activation, **layer_kws)\n",
    "        \n",
    "        layers.append(hidden_layer)\n",
    "        \n",
    "        if use_dropout:\n",
    "            if l==1:\n",
    "                rate = dropout_rate_1\n",
    "            elif l==2 and n_layers > 3:\n",
    "                rate = dropout_rate_2\n",
    "            else:\n",
    "                rate = 0.0\n",
    "            if rate > .0:\n",
    "                layers.append(tf.keras.layers.Dropout(rate=rate))\n",
    "    layers.append(tf.keras.layers.Dense(layer_dims[n_layers-1], activation='linear'))\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, use_ema=use_emma, ema_momentum=emma_momentum),                 \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics='accuracy')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    use_dropout = False # trial.suggest_categorical('use_dropout', [True, False])\n",
    "    dropout_rate_1 = .0 # trial.suggest_float('dropout_rate_1', 0.1, 0.4) if use_dropout else .0\n",
    "    dropout_rate_2 = .0 # trial.suggest_float('dropout_rate_2', 0.0, 0.2) if use_dropout else .0\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [BATCH_SIZE, BATCH_SIZE//2 + 1, BATCH_SIZE//4 + 1])\n",
    "    use_emma = False # trial.suggest_categorical('use_emma', [True, False])\n",
    "    emma_momentum =.0 # trial.suggest_float('emma_momentum', 0.5, 0.9, log=True) if use_emma else 0.999\n",
    "    regularizer = trial.suggest_float('regularizer', 1e-5, 1e-2, log=True)\n",
    "    initializer = trial.suggest_categorical('initializer', ['glorot_normal', 'he_normal'])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'gelu', 'leaky_relu'])\n",
    "\n",
    "    model = build_model(layer_dims=NN_SHAPE, \n",
    "                        use_dropout=use_dropout, dropout_rate_1=dropout_rate_1, dropout_rate_2=dropout_rate_2,\n",
    "                        initializer=initializer,\n",
    "                        regularizer=regularizer, \n",
    "                        activation=activation,\n",
    "                        learning_rate=learning_rate, use_emma=use_emma, emma_momentum=emma_momentum)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "    # print(f'---- step {index+1} of {k}')\n",
    "    # print(f'train size: {len(X_train_set)}, test size: {len(X_test_set)}')\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=MAX_EPOCHS, \n",
    "            validation_data=(X_validation, Y_validation),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0)\n",
    "\n",
    "    Y_predict = sigmoid(model.predict(X_validation))\n",
    "\n",
    "    f1_score = metrics.f1_score(Y_validation, Y_predict)\n",
    "    # print(f\"Validation F1: {f1_score}\")\n",
    "\n",
    "    print(f\"Validation F1 score: {f1_score}\")\n",
    "    # print(f\"Standard deviation of cross-validation score: {tf.math.reduce_std(cvscores)}\")\n",
    "    # print(model.summary())\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name=SCRIPT_NAME\n",
    "storage=f\"sqlite:///{SCRIPT_NAME}.optuna.db\"\n",
    "\n",
    "# recreate study for new NN architecture\n",
    "try:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-08 22:12:07,851] A new study created in RDB with name: DL-07\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 522us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.856742:  10%|█         | 1/10 [00:01<00:11,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.8567415730337078\n",
      "[I 2024-04-08 22:12:09,166] Trial 0 finished with value: 0.8567415730337078 and parameters: {'learning_rate': 0.00031489116479568613, 'batch_size': 6851, 'regularizer': 2.9380279387035334e-05, 'initializer': 'glorot_normal', 'activation': 'relu'}. Best is trial 0 with value: 0.8567415730337078.\n",
      "24/24 [==============================] - 0s 474us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.856742:  20%|██        | 2/10 [00:02<00:10,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.26048565121412803\n",
      "[I 2024-04-08 22:12:10,464] Trial 1 finished with value: 0.26048565121412803 and parameters: {'learning_rate': 1.2087541473056957e-05, 'batch_size': 6851, 'regularizer': 3.511356313970405e-05, 'initializer': 'he_normal', 'activation': 'relu'}. Best is trial 0 with value: 0.8567415730337078.\n",
      "24/24 [==============================] - 0s 521us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.909361:  30%|███       | 3/10 [00:03<00:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.9093610698365527\n",
      "[I 2024-04-08 22:12:11,541] Trial 2 finished with value: 0.9093610698365527 and parameters: {'learning_rate': 0.0028016351587162596, 'batch_size': 1713, 'regularizer': 0.00023345864076016249, 'initializer': 'glorot_normal', 'activation': 'gelu'}. Best is trial 2 with value: 0.9093610698365527.\n",
      "24/24 [==============================] - 0s 469us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.913947:  40%|████      | 4/10 [00:04<00:07,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.913946587537092\n",
      "[I 2024-04-08 22:12:12,751] Trial 3 finished with value: 0.913946587537092 and parameters: {'learning_rate': 0.0026926469100861782, 'batch_size': 1713, 'regularizer': 0.00788671412999049, 'initializer': 'glorot_normal', 'activation': 'gelu'}. Best is trial 3 with value: 0.913946587537092.\n",
      "24/24 [==============================] - 0s 500us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.913947:  50%|█████     | 5/10 [00:06<00:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.49593495934959353\n",
      "[I 2024-04-08 22:12:14,382] Trial 4 finished with value: 0.49593495934959353 and parameters: {'learning_rate': 3.077180271250682e-05, 'batch_size': 1713, 'regularizer': 5.9750279999602906e-05, 'initializer': 'glorot_normal', 'activation': 'gelu'}. Best is trial 3 with value: 0.913946587537092.\n",
      "24/24 [==============================] - 0s 493us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.913947:  60%|██████    | 6/10 [00:07<00:04,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.9112081513828238\n",
      "[I 2024-04-08 22:12:15,123] Trial 5 finished with value: 0.9112081513828238 and parameters: {'learning_rate': 0.07556810141274425, 'batch_size': 3426, 'regularizer': 0.0006218704727769079, 'initializer': 'glorot_normal', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.913946587537092.\n",
      "24/24 [==============================] - 0s 482us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.913947:  70%|███████   | 7/10 [00:08<00:03,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.829971181556196\n",
      "[I 2024-04-08 22:12:16,497] Trial 6 finished with value: 0.829971181556196 and parameters: {'learning_rate': 0.00035868164986275477, 'batch_size': 3426, 'regularizer': 6.963114377829287e-05, 'initializer': 'glorot_normal', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.913946587537092.\n",
      "24/24 [==============================] - 0s 483us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.917031:  80%|████████  | 8/10 [00:09<00:02,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.9170305676855894\n",
      "[I 2024-04-08 22:12:17,353] Trial 7 finished with value: 0.9170305676855894 and parameters: {'learning_rate': 0.012273800987852962, 'batch_size': 1713, 'regularizer': 0.001319994226153501, 'initializer': 'he_normal', 'activation': 'gelu'}. Best is trial 7 with value: 0.9170305676855894.\n",
      "24/24 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.917031:  90%|█████████ | 9/10 [00:10<00:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.9112426035502958\n",
      "[I 2024-04-08 22:12:18,767] Trial 8 finished with value: 0.9112426035502958 and parameters: {'learning_rate': 0.028340904295147733, 'batch_size': 6851, 'regularizer': 8.569331925053983e-05, 'initializer': 'he_normal', 'activation': 'gelu'}. Best is trial 7 with value: 0.9170305676855894.\n",
      "24/24 [==============================] - 0s 889us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.917031: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score: 0.7011494252873564\n",
      "[I 2024-04-08 22:12:21,385] Trial 9 finished with value: 0.7011494252873564 and parameters: {'learning_rate': 3.0086868214458443e-05, 'batch_size': 3426, 'regularizer': 0.002055424552015075, 'initializer': 'he_normal', 'activation': 'relu'}. Best is trial 7 with value: 0.9170305676855894.\n",
      "-> Best score: 0.9170305676855894\n",
      "-> Optimal hyperparameters: \n",
      "{'activation': 'gelu',\n",
      " 'batch_size': 1713,\n",
      " 'initializer': 'he_normal',\n",
      " 'learning_rate': 0.012273800987852962,\n",
      " 'regularizer': 0.001319994226153501}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=study_name, storage=storage,\n",
    "                            direction='maximize', \n",
    "                            sampler=optuna.samplers.TPESampler(seed=42, consider_prior=True),\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# Print optimal hyperparameters and the corresponding score\n",
    "\n",
    "trial = study.best_trial\n",
    "print(f'-> Best score: {trial.value}')\n",
    "print(f'-> Optimal hyperparameters: ')\n",
    "pprint.pprint(trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Best score: 0.9170305676855894\n",
      "{'activation': 'gelu',\n",
      " 'batch_size': 1713,\n",
      " 'initializer': 'he_normal',\n",
      " 'learning_rate': 0.012273800987852962,\n",
      " 'regularizer': 0.001319994226153501}\n"
     ]
    }
   ],
   "source": [
    "print(f'-> Best score: {trial.value}')\n",
    "pprint.pprint(trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "Epoch 2/40\n",
      "Epoch 3/40\n",
      "Epoch 4/40\n",
      "Epoch 5/40\n",
      "Epoch 6/40\n",
      "Epoch 7/40\n",
      "Epoch 8/40\n",
      "Epoch 9/40\n",
      "Epoch 10/40\n",
      "Epoch 11/40\n",
      "Epoch 12/40\n",
      "Epoch 13/40\n",
      "Best model F1=0.918\n"
     ]
    }
   ],
   "source": [
    "def train_best_model(best_params):\n",
    "    batch_size = best_params.pop('batch_size', BATCH_SIZE)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "    best_model = build_model(layer_dims=NN_SHAPE, **best_params)\n",
    "    best_model.fit(X_train, Y_train, batch_size=batch_size, epochs=40, validation_split=0.2,\n",
    "                callbacks=[early_stopping], verbose=3)\n",
    "    Y_predict = sigmoid(best_model(X_train))\n",
    "    f1_score = metrics.f1_score(Y_train, Y_predict)\n",
    "    print(f'Best model F1={f1_score:.3f}')\n",
    "    return best_model\n",
    "\n",
    "best_model = train_best_model(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_132 (Dense)           (None, 32)                5440      \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,041\n",
      "Trainable params: 7,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL-07/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL-07/assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 522us/step\n",
      "F1 score: 0.9069767441860465\n"
     ]
    }
   ],
   "source": [
    "Y_validation_predict = sigmoid(best_model.predict(X_validation))\n",
    "f1_score_test = metrics.f1_score(Y_validation, Y_validation_predict)\n",
    "print(f'F1 score: {f1_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 24)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./test_enriched.csv', index_col='id')\n",
    "df_test.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': '', 'url_domains': ''}, inplace=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (3263, 169)\n"
     ]
    }
   ],
   "source": [
    "X_test = transformer.transform(df_test)\n",
    "print('X_test shape', X_test.shape)\n",
    "\n",
    "Y_test_predict = sigmoid(best_model(X_test))\n",
    "\n",
    "df_example = pd.read_csv('./sample_submission.csv')\n",
    "df_example['target'] = Y_test_predict\n",
    "\n",
    "df_example.to_csv(f'./{SCRIPT_NAME}-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
