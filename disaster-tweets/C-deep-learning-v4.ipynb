{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster tweets DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 20:07:44.952996: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-21 20:07:44.972782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, ensemble, model_selection, pipeline, compose, preprocessing, metrics\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "import tensorflow as tf\n",
    "from embedding_transformer import Doc2VecTransformer\n",
    "\n",
    "SCRIPT_NAME='DL-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>positive_factor</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>missing_location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>...</th>\n",
       "      <th>punct_factor</th>\n",
       "      <th>ann_count</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>stop_words_factor</th>\n",
       "      <th>clean_tokens_factor</th>\n",
       "      <th>url_domains</th>\n",
       "      <th>url_redirects_count</th>\n",
       "      <th>hashtags_sentiment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.590909</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.647059</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword  positive_factor location country state city  missing_location  \\\n",
       "id                                                                          \n",
       "0                       0.5                                             1   \n",
       "1                       0.5                                             1   \n",
       "2                       0.5                                             1   \n",
       "3                       0.5                                             1   \n",
       "4                       0.5                                             1   \n",
       "\n",
       "                                                 text  \\\n",
       "id                                                      \n",
       "0   Our Deeds are the Reason of this #earthquake M...   \n",
       "1              Forest fire near La Ronge Sask. Canada   \n",
       "2   All residents asked to 'shelter in place' are ...   \n",
       "3   13,000 people receive #wildfires evacuation or...   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                           clean_text  text_length  ...  \\\n",
       "id                                                                  ...   \n",
       "0          deed reason earthquake may allah forgive u           57  ...   \n",
       "1               forest fire near la ronge sask canada           32  ...   \n",
       "2   resident asked shelter place notified officer ...          112  ...   \n",
       "3   people receive wildfire evacuation order calif...           57  ...   \n",
       "4   got sent photo ruby alaska smoke wildfire pour...           72  ...   \n",
       "\n",
       "    punct_factor  ann_count  urls_count  tokens_count  stop_words_factor  \\\n",
       "id                                                                         \n",
       "0       0.017544          0           0            13           0.384615   \n",
       "1       0.031250          0           0             7           0.000000   \n",
       "2       0.026786          0           0            22           0.409091   \n",
       "3       0.035088          0           0             9           0.111111   \n",
       "4       0.027778          0           0            17           0.352941   \n",
       "\n",
       "    clean_tokens_factor  url_domains  url_redirects_count hashtags_sentiment  \\\n",
       "id                                                                             \n",
       "0              0.615385                                 0           1.000000   \n",
       "1              1.000000                                 0           0.000000   \n",
       "2              0.590909                                 0           0.000000   \n",
       "3              0.888889                                 0           1.000000   \n",
       "4              0.647059                                 0           0.714286   \n",
       "\n",
       "    target  \n",
       "id          \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_enriched.csv', index_col='id')\n",
    "df_train.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': '', 'url_domains': '', 'clean_text': ''}, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (7613, 1270)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7613, 1270), (7613,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'country',\n",
    "    'state',\n",
    "]\n",
    "numerical_features = [\n",
    "    'text_length', \n",
    "    # 'ann_count',\n",
    "    # 'url_redirects_count',\n",
    "    # 'stop_words_factor',\n",
    "    'positive_factor',\n",
    "    'hashtags_sentiment'\n",
    "]\n",
    "\n",
    "# text_vec = feature_extraction.text.TfidfVectorizer(max_features=2000)\n",
    "# text_vec = Doc2VecTransformer(vector_size=2000)\n",
    "text_vec = feature_extraction.text.CountVectorizer(max_features=1000)\n",
    "# domains_vec = feature_extraction.text.TfidfVectorizer(max_features=100)\n",
    "domains_vec = feature_extraction.text.CountVectorizer(max_features=100)\n",
    "\n",
    "transformer = compose.ColumnTransformer(transformers=[\n",
    "    ('text_vec', text_vec, 'clean_text'),\n",
    "    ('domains_vec', domains_vec, 'url_domains'),\n",
    "    ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('numerical', preprocessing.StandardScaler(), numerical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "X_train = transformer.fit_transform(df_train).todense()\n",
    "print('X_train shape', X_train.shape)\n",
    "\n",
    "Y_train = df_train['target']\n",
    "\n",
    "X_train.shape, Y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)\n",
    "        y_true = tf.cast(y_true, 'float32')  # Convert y_true to float32\n",
    "        y_pred = tf.cast(y_pred, 'float32')  # Ensure y_pred is also float32\n",
    "\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        false_positives = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "        false_negatives = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "\n",
    "        # Update state\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.false_positives.assign_add(false_positives)\n",
    "        self.false_negatives.assign_add(false_negatives)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X_train.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "BATCH_SIZE= 2 # int(0.1*X_train.shape[0])\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def baseline_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, input_shape=(INPUT_SIZE,), activation='relu'),\n",
    "        tf.keras.layers.Dense(int(0.5*HIDDEN_LAYER_SIZE), activation='relu'),\n",
    "        tf.keras.layers.Dense(OUTPUT_SIZE, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, use_ema=True, ema_momentum=0.98),                 \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics='accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 128)               162688    \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,009\n",
      "Trainable params: 171,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/20\n",
      "3045/3045 - 4s - loss: 0.4742 - accuracy: 0.7568 - val_loss: 0.3893 - val_accuracy: 0.8109 - 4s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "3045/3045 - 6s - loss: 0.3594 - accuracy: 0.8407 - val_loss: 0.3738 - val_accuracy: 0.8306 - 6s/epoch - 2ms/step\n",
      "Epoch 3/20\n",
      "3045/3045 - 9s - loss: 0.3156 - accuracy: 0.8608 - val_loss: 0.3784 - val_accuracy: 0.8365 - 9s/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "3045/3045 - 7s - loss: 0.2793 - accuracy: 0.8803 - val_loss: 0.3903 - val_accuracy: 0.8372 - 7s/epoch - 2ms/step\n",
      "F1=0.8754931355531008, accuracy=0.8963614869302509, precision=0.9047619047619048, recall=0.8480586976459799\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "print(model.summary())\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_split=0.2, \n",
    "          callbacks=[early_stopping],\n",
    "          workers=2,\n",
    "          verbose=2)\n",
    "\n",
    "Y_predict = tf.cast(tf.greater(tf.nn.sigmoid(model(X_train)), .5), tf.int32)\n",
    "\n",
    "df_wrong_predictions = pd.DataFrame({'target': tf.squeeze(Y_train), 'predict': tf.squeeze(Y_predict), 'keyword': df_train['keyword'], 'location': df_train['location'], 'text': df_train['text']}).query('target != predict')\n",
    "\n",
    "f1_score = metrics.f1_score(Y_train, Y_predict)\n",
    "accuracy = metrics.accuracy_score(Y_train, Y_predict)\n",
    "precision = metrics.precision_score(Y_train, Y_predict)\n",
    "recall = metrics.recall_score(Y_train, Y_predict)\n",
    "\n",
    "print(f'F1={f1_score}, accuracy={accuracy}, precision={precision}, recall={recall}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.47814974188804626,\n",
       "  0.3706640899181366,\n",
       "  0.3284318745136261,\n",
       "  0.28984159231185913,\n",
       "  0.2545904517173767,\n",
       "  0.22112438082695007],\n",
       " 'accuracy': [0.7435139417648315,\n",
       "  0.832348108291626,\n",
       "  0.8538587689399719,\n",
       "  0.8750410676002502,\n",
       "  0.8896551728248596,\n",
       "  0.9100164175033569],\n",
       " 'val_loss': [0.40050220489501953,\n",
       "  0.38775569200515747,\n",
       "  0.38331034779548645,\n",
       "  0.3898771107196808,\n",
       "  0.4109799563884735,\n",
       "  0.43590298295021057],\n",
       " 'val_accuracy': [0.8220617175102234,\n",
       "  0.8312541246414185,\n",
       "  0.829940915107727,\n",
       "  0.8325672745704651,\n",
       "  0.8253447413444519,\n",
       "  0.8240315318107605]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target  predict      keyword         location  \\\n",
      "id                                                    \n",
      "4698       1        0    landslide                    \n",
      "7231       1        0      weapons                    \n",
      "7242       1        0      weapons    Hawthorne, NE   \n",
      "6829       1        0      trapped     876 Jamrock.   \n",
      "2905       1        0        drown         Portugal   \n",
      "2085       1        0         dead       Spare 'Oom   \n",
      "2033       1        0       danger  Lahar & Gwalior   \n",
      "2554       1        0      destroy  Jerseyville, IL   \n",
      "300        1        0   apocalypse                    \n",
      "229        1        0  annihilated                    \n",
      "\n",
      "                                                                                                                                                 text  \n",
      "id                                                                                                                                                     \n",
      "4698                                                 Method in contemplation of incident an leading bridal landslide: wiWNpFXA http://t.co/xysNXUM29T  \n",
      "7231                            @NRO Except when ordered not to carry unauthorized weapons while on duty or in military uniforms. THATS THE RULE FOOL  \n",
      "7242                                              PM Abe pledged to make every effort to seek a world without nuclear weapons. http://t.co/CBXnHhZ6kD  \n",
      "6829                                                  Literally trapped in my room Cuz my bathroom being remodeled. The only exit is through a window  \n",
      "2905                                                                                                    I can't drown my demons they know how to swim  \n",
      "2085                                                                                                                   that's it val is dead im suing  \n",
      "2033                         Indian Govt. &amp; Media should take serious concern about their safety. They are in danger now. https://t.co/YX1UKbmTqB  \n",
      "2554  Dem FLATLINERS who destroy creativity-balance-longevity &amp; TRUTH stand with Lucifer in all his flames of destruction https://t.co/WcFpZNsN9u  \n",
      "300                                                                                                                      Shadow boxing the apocalypse  \n",
      "229                                                                                                        Ready to get annihilated for the BUCS game  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 200):\n",
    "    print(df_wrong_predictions.query('target==1').sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL-04/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL-04/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (3263, 1273)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('./test_enriched.csv', index_col='id')\n",
    "df_test.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': '', 'url_domains': ''}, inplace=True)\n",
    "X_test = transformer.transform(df_test).todense()\n",
    "print('X_test shape', X_test.shape)\n",
    "\n",
    "Y_test_predict = tf.cast(tf.greater(tf.nn.sigmoid(model(X_test)), .5), tf.int32)\n",
    "\n",
    "df_example = pd.read_csv('./sample_submission.csv')\n",
    "df_example['target'] = Y_test_predict\n",
    "\n",
    "df_example.to_csv(f'./{SCRIPT_NAME}-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
