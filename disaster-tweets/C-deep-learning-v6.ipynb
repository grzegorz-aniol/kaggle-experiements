{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster tweets DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import base, feature_extraction, ensemble, model_selection, pipeline, compose, preprocessing, metrics\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "import tensorflow as tf\n",
    "from embedding_transformer import Doc2VecTransformer\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import optuna\n",
    "import pprint\n",
    "\n",
    "SCRIPT_NAME='DL-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>positive_factor</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>missing_location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_content</th>\n",
       "      <th>...</th>\n",
       "      <th>punct_factor</th>\n",
       "      <th>ann_count</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>stop_words_factor</th>\n",
       "      <th>clean_tokens_factor</th>\n",
       "      <th>url_domains</th>\n",
       "      <th>url_redirects_count</th>\n",
       "      <th>hashtags_sentiment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.590909</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.647059</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword  positive_factor location country state city  missing_location  \\\n",
       "id                                                                          \n",
       "0                       0.5                                             1   \n",
       "1                       0.5                                             1   \n",
       "2                       0.5                                             1   \n",
       "3                       0.5                                             1   \n",
       "4                       0.5                                             1   \n",
       "\n",
       "                                                 text  \\\n",
       "id                                                      \n",
       "0   Our Deeds are the Reason of this #earthquake M...   \n",
       "1              Forest fire near La Ronge Sask. Canada   \n",
       "2   All residents asked to 'shelter in place' are ...   \n",
       "3   13,000 people receive #wildfires evacuation or...   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                           clean_text  \\\n",
       "id                                                      \n",
       "0          deed reason earthquake may allah forgive u   \n",
       "1               forest fire near la ronge sask canada   \n",
       "2   resident asked shelter place notified officer ...   \n",
       "3   people receive wildfire evacuation order calif...   \n",
       "4   got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                         text_content  ...  punct_factor  \\\n",
       "id                                                     ...                 \n",
       "0   Our Deeds are the Reason of this #earthquake M...  ...      0.017544   \n",
       "1              Forest fire near La Ronge Sask. Canada  ...      0.031250   \n",
       "2   All residents asked to 'shelter in place' are ...  ...      0.026786   \n",
       "3   13,000 people receive #wildfires evacuation or...  ...      0.035088   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...  ...      0.027778   \n",
       "\n",
       "    ann_count  urls_count  tokens_count  stop_words_factor  \\\n",
       "id                                                           \n",
       "0           0           0            13           0.384615   \n",
       "1           0           0             7           0.000000   \n",
       "2           0           0            22           0.409091   \n",
       "3           0           0             9           0.111111   \n",
       "4           0           0            17           0.352941   \n",
       "\n",
       "    clean_tokens_factor  url_domains  url_redirects_count  hashtags_sentiment  \\\n",
       "id                                                                              \n",
       "0              0.615385                                 0            1.000000   \n",
       "1              1.000000                                 0            0.000000   \n",
       "2              0.590909                                 0            0.000000   \n",
       "3              0.888889                                 0            1.000000   \n",
       "4              0.647059                                 0            0.714286   \n",
       "\n",
       "   target  \n",
       "id         \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_enriched.csv', index_col='id')\n",
    "df_train.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': '', 'url_domains': '', 'clean_text': ''}, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = None\n",
    "with open('./train-text-embeddings.pkl', 'rb') as fin:\n",
    "    text_embedding = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 384)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_embedding), len(text_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalEmbeddingTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    def __init__(self, data):\n",
    "        # Store the embeddings and mode\n",
    "        self.data = data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary, return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.concatenate([X, self.data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (7613, 554)\n",
      "X_train type=<class 'numpy.ndarray'>, shape=(7613, 554)\n",
      "Y_train shape=(7613,)\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'country',\n",
    "    'state',\n",
    "]\n",
    "numerical_features = [\n",
    "    'text_length', \n",
    "    # 'ann_count',\n",
    "    # 'url_redirects_count',\n",
    "    # 'stop_words_factor',\n",
    "    'positive_factor',\n",
    "    'hashtags_sentiment'\n",
    "]\n",
    "\n",
    "# domains_vec = feature_extraction.text.TfidfVectorizer(max_features=100)\n",
    "# domains_vec = feature_extraction.text.CountVectorizer(max_features=100)\n",
    "\n",
    "column_transformer = compose.ColumnTransformer(transformers=[\n",
    "    # ('domains_vec', domains_vec, 'url_domains'),\n",
    "    ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('numerical', preprocessing.StandardScaler(), numerical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "embedding_transformer = ConditionalEmbeddingTransformer(text_embedding)\n",
    "\n",
    "transformer = pipeline.Pipeline([\n",
    "    ('columns', column_transformer),\n",
    "    ('text_embedding', embedding_transformer)\n",
    "])\n",
    "\n",
    "transformer.fit(df_train)\n",
    "X_train = transformer.transform(df_train)\n",
    "print('X_train shape', X_train.shape)\n",
    "\n",
    "Y_train = df_train['target']\n",
    "\n",
    "print(f'X_train type={type(X_train)}, shape={X_train.shape}')\n",
    "print(f'Y_train shape={Y_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X_train.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "NN_SHAPE = [INPUT_SIZE, 256, 256, 64, 1]\n",
    "BATCH_SIZE= 8\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return tf.cast(tf.greater(tf.nn.sigmoid(x), .5), tf.int32)\n",
    "\n",
    "\n",
    "def build_model(layer_dims, use_dropout=False, dropout_rate_1=0.3, dropout_rate_2=0.1,\n",
    "                learning_rate=1e-3, \n",
    "                use_emma=False, emma_momentum=0.99, regularizer=0.01):\n",
    "    n_layers = len(layer_dims)\n",
    "    layers = []\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    for l in range(1, n_layers-1):\n",
    "        tf.keras.layers.Dense(layer_dims[l], input_shape=(layer_dims[l-1],), activation='relu', \n",
    "                              kernel_initializer=initializer, kernel_regularizer=tf.keras.regularizers.l2(regularizer))\n",
    "        if use_dropout:\n",
    "            if l==1:\n",
    "                rate = dropout_rate_1\n",
    "            elif l==2 and n_layers > 3:\n",
    "                rate = dropout_rate_2\n",
    "            else:\n",
    "                rate = 0.0\n",
    "            layers.append(tf.keras.layers.Dropout(rate=rate))\n",
    "    layers.append(tf.keras.layers.Dense(layer_dims[n_layers-1], activation='linear'))\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, use_ema=use_emma, ema_momentum=emma_momentum),                 \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics='accuracy')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    use_dropout = trial.suggest_categorical('use_dropout', [True, False])\n",
    "    dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.4) if use_dropout else .0\n",
    "    dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.0, 0.2) if use_dropout else .0\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [2, 4, 8, 16])\n",
    "    use_emma = False # trial.suggest_categorical('use_emma', [True, False])\n",
    "    emma_momentum = .0 # trial.suggest_float('emma_momentum', 0.9, 0.9999, log=True) if use_emma else 0.999\n",
    "    regularizer = False # trial.suggest_float('regularizer', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    k = 3  # Number of validations\n",
    "    shuffle_split = model_selection.StratifiedShuffleSplit(n_splits=k, test_size=0.2)\n",
    "    cvscores = []\n",
    "\n",
    "    model = build_model(layer_dims=NN_SHAPE, \n",
    "                        use_dropout=use_dropout, dropout_rate_1=dropout_rate_1, dropout_rate_2=dropout_rate_2,\n",
    "                        regularizer=regularizer, \n",
    "                        learning_rate=learning_rate, use_emma=use_emma, emma_momentum=emma_momentum)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "    for index, datasets in enumerate(shuffle_split.split(X_train, Y_train)):\n",
    "        train, test = datasets\n",
    "        X_train_set = X_train[train]\n",
    "        Y_train_set = Y_train[train]\n",
    "        X_test_set = X_train[test]\n",
    "        Y_test_set = Y_train[test]\n",
    "        # print(f'---- step {index+1} of {k}')\n",
    "        # print(f'train size: {len(X_train_set)}, test size: {len(X_test_set)}')\n",
    "        \n",
    "        model.fit(X_train_set, Y_train_set, batch_size=batch_size, epochs=MAX_EPOCHS, \n",
    "                validation_data=(X_test_set, Y_test_set),\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=0)\n",
    "\n",
    "        Y_predict = sigmoid(model.predict(X_test_set))\n",
    "\n",
    "        f1_score = metrics.f1_score(Y_test_set, Y_predict)\n",
    "        # print(f\"Validation F1: {f1_score}\")\n",
    "\n",
    "        cvscores.append(f1_score)\n",
    "\n",
    "    score = np.mean(cvscores)\n",
    "    print(f\"Mean cross-validation F1 score: {score}\")\n",
    "    # print(f\"Standard deviation of cross-validation score: {tf.math.reduce_std(cvscores)}\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name=SCRIPT_NAME\n",
    "storage=f\"sqlite:///{SCRIPT_NAME}.optuna.db\"\n",
    "\n",
    "# recreate study for new NN architecture\n",
    "try:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 09:08:06,642] A new study created in RDB with name: DL-06\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 833us/step\n",
      "48/48 [==============================] - 0s 867us/step\n",
      "48/48 [==============================] - 0s 750us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  10%|█         | 1/10 [00:26<03:58, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8322304785845218\n",
      "[I 2024-04-02 09:08:33,128] Trial 0 finished with value: 0.8322304785845218 and parameters: {'use_dropout': False, 'learning_rate': 0.008471801418819975, 'batch_size': 2}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 1ms/step\n",
      "48/48 [==============================] - 0s 2ms/step\n",
      "48/48 [==============================] - 0s 798us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  20%|██        | 2/10 [01:07<04:38, 34.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8067718252186937\n",
      "[I 2024-04-02 09:09:13,869] Trial 1 finished with value: 0.8067718252186937 and parameters: {'use_dropout': True, 'dropout_rate_1': 0.3124217733388137, 'dropout_rate_2': 0.004116898859160489, 'learning_rate': 0.07579479953348005, 'batch_size': 2}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 728us/step\n",
      "48/48 [==============================] - 0s 679us/step\n",
      "48/48 [==============================] - 0s 615us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  30%|███       | 3/10 [01:54<04:42, 40.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8280592105022032\n",
      "[I 2024-04-02 09:10:00,800] Trial 2 finished with value: 0.8280592105022032 and parameters: {'use_dropout': False, 'learning_rate': 0.0005342937261279777, 'batch_size': 4}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 834us/step\n",
      "48/48 [==============================] - 0s 835us/step\n",
      "48/48 [==============================] - 0s 720us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  40%|████      | 4/10 [02:03<02:47, 27.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8226004414181091\n",
      "[I 2024-04-02 09:10:09,711] Trial 3 finished with value: 0.8226004414181091 and parameters: {'use_dropout': False, 'learning_rate': 0.013826232179369865, 'batch_size': 8}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 875us/step\n",
      "48/48 [==============================] - 0s 750us/step\n",
      "48/48 [==============================] - 0s 430us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  50%|█████     | 5/10 [02:53<03:00, 36.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8012561601715259\n",
      "[I 2024-04-02 09:11:00,467] Trial 4 finished with value: 0.8012561601715259 and parameters: {'use_dropout': True, 'dropout_rate_1': 0.11951547789558387, 'dropout_rate_2': 0.18977710745066667, 'learning_rate': 0.07286653737491042, 'batch_size': 2}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 879us/step\n",
      "48/48 [==============================] - 0s 945us/step\n",
      "48/48 [==============================] - 0s 881us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  60%|██████    | 6/10 [03:14<02:03, 31.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8182585856867198\n",
      "[I 2024-04-02 09:11:21,404] Trial 5 finished with value: 0.8182585856867198 and parameters: {'use_dropout': True, 'dropout_rate_1': 0.2485530730333811, 'dropout_rate_2': 0.006877704223043679, 'learning_rate': 0.0433792069749094, 'batch_size': 4}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 794us/step\n",
      "48/48 [==============================] - 0s 785us/step\n",
      "48/48 [==============================] - 0s 792us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  70%|███████   | 7/10 [03:25<01:13, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8022261107980642\n",
      "[I 2024-04-02 09:11:32,171] Trial 6 finished with value: 0.8022261107980642 and parameters: {'use_dropout': True, 'dropout_rate_1': 0.39087538832936763, 'dropout_rate_2': 0.15502656467222292, 'learning_rate': 0.057279044707996205, 'batch_size': 8}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 830us/step\n",
      "48/48 [==============================] - 0s 778us/step\n",
      "48/48 [==============================] - 0s 832us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  80%|████████  | 8/10 [05:56<02:09, 64.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.8262679449822562\n",
      "[I 2024-04-02 09:14:03,179] Trial 7 finished with value: 0.8262679449822562 and parameters: {'use_dropout': True, 'dropout_rate_1': 0.19759909922897934, 'dropout_rate_2': 0.07773545793789641, 'learning_rate': 0.00012172958098369953, 'batch_size': 2}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 725us/step\n",
      "48/48 [==============================] - 0s 688us/step\n",
      "48/48 [==============================] - 0s 792us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223:  90%|█████████ | 9/10 [09:02<01:42, 102.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.7825249394868652\n",
      "[I 2024-04-02 09:17:09,304] Trial 8 finished with value: 0.7825249394868652 and parameters: {'use_dropout': False, 'learning_rate': 1.9870215385428627e-05, 'batch_size': 2}. Best is trial 0 with value: 0.8322304785845218.\n",
      "48/48 [==============================] - 0s 810us/step\n",
      "48/48 [==============================] - 0s 2ms/step\n",
      "48/48 [==============================] - 0s 999us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.83223: 100%|██████████| 10/10 [10:02<00:00, 60.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation F1 score: 0.7657339541864617\n",
      "[I 2024-04-02 09:18:08,653] Trial 9 finished with value: 0.7657339541864617 and parameters: {'use_dropout': True, 'dropout_rate_1': 0.3187021504122962, 'dropout_rate_2': 0.15425406933718916, 'learning_rate': 1.97778285124627e-05, 'batch_size': 8}. Best is trial 0 with value: 0.8322304785845218.\n",
      "-> Best score: 0.8322304785845218\n",
      "-> Optimal hyperparameters: \n",
      "{'batch_size': 2, 'learning_rate': 0.008471801418819975, 'use_dropout': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=study_name, storage=storage,\n",
    "                            direction='maximize', \n",
    "                            sampler=optuna.samplers.TPESampler(seed=42, consider_prior=True),\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# Print optimal hyperparameters and the corresponding score\n",
    "\n",
    "trial = study.best_trial\n",
    "print(f'-> Best score: {trial.value}')\n",
    "print(f'-> Optimal hyperparameters: ')\n",
    "pprint.pprint(trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Best score: 0.833765885392289\n",
      "{'batch_size': 4, 'learning_rate': 0.004835952776465951, 'use_dropout': False}\n"
     ]
    }
   ],
   "source": [
    "print(f'-> Best score: {trial.value}')\n",
    "pprint.pprint(trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "Best model F1=0.826\n"
     ]
    }
   ],
   "source": [
    "def train_best_model(best_params):\n",
    "    batch_size = best_params.pop('batch_size', BATCH_SIZE)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "    best_model = build_model(layer_dims=NN_SHAPE, **best_params)\n",
    "    best_model.fit(X_train, Y_train, batch_size=batch_size, epochs=MAX_EPOCHS, validation_split=0.2,\n",
    "                callbacks=[early_stopping], verbose=3)\n",
    "    Y_predict = sigmoid(best_model(X_train))\n",
    "    f1_score = metrics.f1_score(Y_train, Y_predict)\n",
    "    print(f'Best model F1={f1_score:.3f}')\n",
    "    return best_model\n",
    "\n",
    "best_model = train_best_model(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 1024)              568320    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,618,945\n",
      "Trainable params: 1,618,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL-06/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DL-06/assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 22)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./test_enriched.csv', index_col='id')\n",
    "df_test.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': '', 'url_domains': ''}, inplace=True)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 384)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = None\n",
    "with open('./test-text-embeddings.pkl', 'rb') as fin:\n",
    "    test_embedding = pickle.load(fin)\n",
    "len(test_embedding), len(test_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (3263, 554)\n"
     ]
    }
   ],
   "source": [
    "embedding_transformer.data = test_embedding\n",
    "X_test = transformer.transform(df_test)\n",
    "print('X_test shape', X_test.shape)\n",
    "\n",
    "Y_test_predict = sigmoid(best_model(X_test))\n",
    "\n",
    "df_example = pd.read_csv('./sample_submission.csv')\n",
    "df_example['target'] = Y_test_predict\n",
    "\n",
    "df_example.to_csv(f'./{SCRIPT_NAME}-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
