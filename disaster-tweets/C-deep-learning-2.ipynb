{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster tweets DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 17:59:02.759892: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-18 17:59:02.779487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, ensemble, model_selection, pipeline, compose, preprocessing, metrics\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "import tensorflow as tf\n",
    "from embedding_transformer import Doc2VecTransformer\n",
    "\n",
    "SCRIPT_NAME='DL-02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>positive_factor</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>missing_location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>upper_text_factor</th>\n",
       "      <th>tags_count</th>\n",
       "      <th>punct_factor</th>\n",
       "      <th>ann_count</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>stop_words_factor</th>\n",
       "      <th>clean_tokens_factor</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>57</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>32</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>112</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>72</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword  positive_factor location country state city  missing_location  \\\n",
       "id                                                                          \n",
       "0                       0.5                                             1   \n",
       "1                       0.5                                             1   \n",
       "2                       0.5                                             1   \n",
       "3                       0.5                                             1   \n",
       "4                       0.5                                             1   \n",
       "\n",
       "                                                 text  \\\n",
       "id                                                      \n",
       "0   Our Deeds are the Reason of this #earthquake M...   \n",
       "1              Forest fire near La Ronge Sask. Canada   \n",
       "2   All residents asked to 'shelter in place' are ...   \n",
       "3   13,000 people receive #wildfires evacuation or...   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                           clean_text  text_length  \\\n",
       "id                                                                   \n",
       "0          deed reason earthquake may allah forgive u           57   \n",
       "1               forest fire near la ronge sask canada           32   \n",
       "2   resident asked shelter place notified officer ...          112   \n",
       "3   people receive wildfire evacuation order calif...           57   \n",
       "4   got sent photo ruby alaska smoke wildfire pour...           72   \n",
       "\n",
       "    upper_text_factor  tags_count  punct_factor  ann_count  urls_count  \\\n",
       "id                                                                       \n",
       "0            0.175439           1      0.017544          0           0   \n",
       "1            0.156250           0      0.031250          0           0   \n",
       "2            0.017857           0      0.026786          0           0   \n",
       "3            0.017544           1      0.035088          0           0   \n",
       "4            0.041667           2      0.027778          0           0   \n",
       "\n",
       "    tokens_count  stop_words_factor  clean_tokens_factor  target  \n",
       "id                                                                \n",
       "0             13           0.384615             0.615385       1  \n",
       "1              7           0.000000             1.000000       1  \n",
       "2             22           0.409091             0.590909       1  \n",
       "3              9           0.111111             0.888889       1  \n",
       "4             17           0.352941             0.647059       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_enriched.csv', index_col='id')\n",
    "df_train.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': ''}, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (7613, 2177)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7613, 2177), (7613,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'country',\n",
    "    'state',\n",
    "]\n",
    "numerical_features = [\n",
    "    'text_length', \n",
    "    'urls_count',\n",
    "    'stop_words_factor',\n",
    "    'clean_tokens_factor'\n",
    "]\n",
    "\n",
    "# vc_text = feature_extraction.text.TfidfVectorizer(max_features=2000)\n",
    "doc2vec = Doc2VecTransformer(vector_size=2000)\n",
    "\n",
    "transformer = compose.ColumnTransformer(transformers=[\n",
    "    # ('text_vector', vc_text, 'clean_text'),\n",
    "    ('doc2vec', doc2vec, 'clean_text'),\n",
    "    ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('numerical', preprocessing.StandardScaler(), numerical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "X_train = transformer.fit_transform(df_train)\n",
    "print('X_train shape', X_train.shape)\n",
    "\n",
    "Y_train = df_train['target']\n",
    "\n",
    "X_train.shape, Y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X_train.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "HIDDEN_LAYER_SIZE = 4096\n",
    "BATCH_SIZE= 64\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "def baseline_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2*HIDDEN_LAYER_SIZE, input_shape=(INPUT_SIZE,), activation='relu'),\n",
    "        tf.keras.layers.Dense(2*HIDDEN_LAYER_SIZE, activation='relu'),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation='relu'),\n",
    "        tf.keras.layers.Dense(OUTPUT_SIZE, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics='accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "96/96 - 100s - loss: 0.5626 - accuracy: 0.7374 - val_loss: 0.4741 - val_accuracy: 0.7649 - 100s/epoch - 1s/step\n",
      "Epoch 2/200\n",
      "96/96 - 88s - loss: 0.4814 - accuracy: 0.7640 - val_loss: 0.5274 - val_accuracy: 0.7433 - 88s/epoch - 921ms/step\n",
      "Epoch 3/200\n",
      "96/96 - 86s - loss: 0.4668 - accuracy: 0.7698 - val_loss: 0.4827 - val_accuracy: 0.7840 - 86s/epoch - 892ms/step\n",
      "Epoch 4/200\n",
      "96/96 - 91s - loss: 0.4461 - accuracy: 0.7834 - val_loss: 0.4829 - val_accuracy: 0.7636 - 91s/epoch - 945ms/step\n",
      "Epoch 5/200\n",
      "96/96 - 93s - loss: 0.4375 - accuracy: 0.7875 - val_loss: 0.4903 - val_accuracy: 0.7754 - 93s/epoch - 964ms/step\n",
      "F1=0.7238222074246713, accuracy=0.7820832786023907, precision=0.7945906432748538\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_split=0.2, \n",
    "          callbacks=[early_stopping],\n",
    "          verbose=2)\n",
    "\n",
    "Y_predict = tf.cast(tf.greater(tf.nn.sigmoid(model(X_train)), .5), tf.int32)\n",
    "\n",
    "df_wrong_predictions = pd.DataFrame({'target': tf.squeeze(Y_train), 'predict': tf.squeeze(Y_predict), 'keyword': df_train['keyword'], 'location': df_train['location'], 'text': df_train['text']}).query('target != predict')\n",
    "\n",
    "f1_score = metrics.f1_score(Y_train, Y_predict)\n",
    "accuracy = metrics.accuracy_score(Y_train, Y_predict)\n",
    "precision = metrics.precision_score(Y_train, Y_predict)\n",
    "\n",
    "print(f'F1={f1_score}, accuracy={accuracy}, precision={precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target  predict            keyword                      location  \\\n",
      "id                                                                       \n",
      "5122       1        0  nuclear%20reactor                                 \n",
      "4318       1        0           hellfire                                 \n",
      "1279       1        0             burned                   Oakland, CA   \n",
      "1624       1        0           collapse                United Kingdom   \n",
      "6844       1        0             trauma                                 \n",
      "6030       1        0            seismic                        ??????   \n",
      "2220       1        0             deluge  Enniscrone & Aughris, Sligo    \n",
      "317        1        0         armageddon     California, United States   \n",
      "2009       1        0             damage  261 5th Avenue New York, NY    \n",
      "3519       1        0         eyewitness                     Stay Fly?   \n",
      "\n",
      "                                                                                                                                              text  \n",
      "id                                                                                                                                                  \n",
      "5122     US Navy Sidelines 3 Newest Subs - http://t.co/guvTIzyCHE: DefenseNews.comUS Navy Sidelines 3 Newest SubsD... http://t.co/SY2WhXT0K5 #navy  \n",
      "4318                            The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'  \n",
      "1279                                                                       Burned dog finds new home with young burn victim http://t.co/Pqrjvgvgxg  \n",
      "1624                                                                                         Now that's what you call a batting collapse #theashes  \n",
      "6844         Photo: lavenderpoetrycafe: Trauma memories are encoded in images as trauma is a more sensory than cognitive... http://t.co/DMb6xP966D  \n",
      "6030                                         [Report 5] 18:22:45 Ibaraki Prefecture offing M5.5 Depth 60km Maximum seismic intensity 4 #Earthquake  \n",
      "2220          Back on the beach after the deluge.  Surf camp in motion.  Our Surf Therapy programme kicked off today for... http://t.co/vjsAqPxngN  \n",
      "317                                             #PBBan (Temporary:300) hyider_ghost2 @'aRmageddon | DO NOT KILL | FLAGS ONLY | Fast XP' for Reason  \n",
      "2009                Does homeowners insurance cover water damage? Here are some good things to know. http://t.co/0uSDI5JCHo http://t.co/xyg7JhRjoF  \n",
      "3519  Lone Survivor: The Eyewitness Account of Operation Redwing and the Lost Heroes of SEAL TeamÛ_ http://t.co/NXtWXJCAVh http://t.co/oL8ESFRGLE  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 200):\n",
    "    print(df_wrong_predictions.query('target==1').sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C-DL-1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C-DL-1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(SCRIPT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (3263, 2172)\n"
     ]
    }
   ],
   "source": [
    "X_test = transformer.transform(pd.read_csv('./test_enriched.csv', index_col='id')).todense()\n",
    "print('X_test shape', X_test.shape)\n",
    "\n",
    "Y_test_predict = tf.cast(tf.greater(tf.nn.sigmoid(model(X_test)), .5), tf.int32)\n",
    "\n",
    "df_example = pd.read_csv('./sample_submission.csv')\n",
    "df_example['target'] = Y_test_predict\n",
    "\n",
    "df_example.to_csv(f'./{SCRIPT_NAME}-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
