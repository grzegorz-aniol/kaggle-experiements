{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster tweets DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, ensemble, model_selection, pipeline, compose, preprocessing, metrics\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "import tensorflow as tf\n",
    "SCRIPT_NAME='DL-02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>positive_factor</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>missing_location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>upper_text_factor</th>\n",
       "      <th>tags_count</th>\n",
       "      <th>punct_factor</th>\n",
       "      <th>ann_count</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>tokens_count</th>\n",
       "      <th>stop_words_factor</th>\n",
       "      <th>clean_tokens_factor</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>57</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>32</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>112</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>72</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword  positive_factor location country state city  missing_location  \\\n",
       "id                                                                          \n",
       "0                       0.5                                             1   \n",
       "1                       0.5                                             1   \n",
       "2                       0.5                                             1   \n",
       "3                       0.5                                             1   \n",
       "4                       0.5                                             1   \n",
       "\n",
       "                                                 text  \\\n",
       "id                                                      \n",
       "0   Our Deeds are the Reason of this #earthquake M...   \n",
       "1              Forest fire near La Ronge Sask. Canada   \n",
       "2   All residents asked to 'shelter in place' are ...   \n",
       "3   13,000 people receive #wildfires evacuation or...   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                           clean_text  text_length  \\\n",
       "id                                                                   \n",
       "0          deed reason earthquake may allah forgive u           57   \n",
       "1               forest fire near la ronge sask canada           32   \n",
       "2   resident asked shelter place notified officer ...          112   \n",
       "3   people receive wildfire evacuation order calif...           57   \n",
       "4   got sent photo ruby alaska smoke wildfire pour...           72   \n",
       "\n",
       "    upper_text_factor  tags_count  punct_factor  ann_count  urls_count  \\\n",
       "id                                                                       \n",
       "0            0.175439           1      0.017544          0           0   \n",
       "1            0.156250           0      0.031250          0           0   \n",
       "2            0.017857           0      0.026786          0           0   \n",
       "3            0.017544           1      0.035088          0           0   \n",
       "4            0.041667           2      0.027778          0           0   \n",
       "\n",
       "    tokens_count  stop_words_factor  clean_tokens_factor  target  \n",
       "id                                                                \n",
       "0             13           0.384615             0.615385       1  \n",
       "1              7           0.000000             1.000000       1  \n",
       "2             22           0.409091             0.590909       1  \n",
       "3              9           0.111111             0.888889       1  \n",
       "4             17           0.352941             0.647059       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_enriched.csv', index_col='id')\n",
    "df_train.fillna({'keyword': '', 'location': '', 'country': '', 'state': '', 'city': ''}, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (7613, 2172)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7613, 2172), (7613,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'country',\n",
    "    'state',\n",
    "]\n",
    "numerical_features = [\n",
    "    'text_length', \n",
    "    'urls_count',\n",
    "    'stop_words_factor',\n",
    "    'clean_tokens_factor',\n",
    "    'positive_factor'\n",
    "]\n",
    "\n",
    "vc_text = feature_extraction.text.TfidfVectorizer(max_features=2000)\n",
    "\n",
    "transformer = compose.ColumnTransformer(transformers=[\n",
    "    ('text_vector', vc_text, 'clean_text'),\n",
    "    ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('numerical', preprocessing.StandardScaler(), numerical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "X_train = transformer.fit_transform(df_train).todense()\n",
    "print('X_train shape', X_train.shape)\n",
    "\n",
    "Y_train = df_train['target']\n",
    "\n",
    "X_train.shape, Y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = X_train.shape[1]\n",
    "OUTPUT_SIZE = 1\n",
    "HIDDEN_LAYER_SIZE = 1024\n",
    "BATCH_SIZE= 64\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "def baseline_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2*HIDDEN_LAYER_SIZE, input_shape=(INPUT_SIZE,), activation='relu'),\n",
    "        tf.keras.layers.Dense(2*HIDDEN_LAYER_SIZE, activation='relu'),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation='relu'),\n",
    "        tf.keras.layers.Dense(OUTPUT_SIZE, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics='accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "96/96 - 7s - loss: 0.4968 - accuracy: 0.7573 - val_loss: 0.4800 - val_accuracy: 0.7452 - 7s/epoch - 73ms/step\n",
      "Epoch 2/200\n",
      "96/96 - 9s - loss: 0.3424 - accuracy: 0.8440 - val_loss: 0.4415 - val_accuracy: 0.8076 - 9s/epoch - 95ms/step\n",
      "Epoch 3/200\n",
      "96/96 - 9s - loss: 0.2000 - accuracy: 0.9174 - val_loss: 0.6120 - val_accuracy: 0.7794 - 9s/epoch - 90ms/step\n",
      "Epoch 4/200\n",
      "96/96 - 8s - loss: 0.1024 - accuracy: 0.9599 - val_loss: 0.7085 - val_accuracy: 0.7807 - 8s/epoch - 85ms/step\n",
      "Epoch 5/200\n",
      "96/96 - 8s - loss: 0.0775 - accuracy: 0.9678 - val_loss: 1.0827 - val_accuracy: 0.7905 - 8s/epoch - 82ms/step\n",
      "Epoch 6/200\n",
      "96/96 - 8s - loss: 0.0559 - accuracy: 0.9752 - val_loss: 1.0593 - val_accuracy: 0.7853 - 8s/epoch - 83ms/step\n",
      "F1=0.8888186986734049, accuracy=0.9075265992381453, precision=0.9193074158771644\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_split=0.2, \n",
    "          callbacks=[early_stopping],\n",
    "          verbose=2)\n",
    "\n",
    "Y_predict = tf.cast(tf.greater(tf.nn.sigmoid(model(X_train)), .5), tf.int32)\n",
    "\n",
    "df_wrong_predictions = pd.DataFrame({'target': tf.squeeze(Y_train), 'predict': tf.squeeze(Y_predict), 'keyword': df_train['keyword'], 'location': df_train['location'], 'text': df_train['text']}).query('target != predict')\n",
    "\n",
    "f1_score = metrics.f1_score(Y_train, Y_predict)\n",
    "accuracy = metrics.accuracy_score(Y_train, Y_predict)\n",
    "precision = metrics.precision_score(Y_train, Y_predict)\n",
    "\n",
    "print(f'F1={f1_score}, accuracy={accuracy}, precision={precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target  predict            keyword                  location  \\\n",
      "id                                                                   \n",
      "2066       0        1               dead                             \n",
      "7386       0        1          windstorm                   Houston   \n",
      "6868       0        1             trauma                             \n",
      "7317       0        1       wild%20fires      West Vancouver, B.C.   \n",
      "2546       0        1            destroy             Johannesburg    \n",
      "1190       0        1  bridge%20collapse                 Leicester   \n",
      "4344       0        1             hijack                   Nigeria   \n",
      "2389       0        1             derail           London, England   \n",
      "2503       0        1           desolate                             \n",
      "7092       0        1           upheaval  Perth, Western Australia   \n",
      "\n",
      "                                                                                                                                                text  \n",
      "id                                                                                                                                                    \n",
      "2066                                                                We just happened to get on the same road right behind the buses I'm dead serious  \n",
      "7386                                                                     New roof and hardy up..Windstorm inspection tomorrow http://t.co/kKeH8qCgc3  \n",
      "6868                                                            What is the role of usg in paeds major trauma imaging decision tool? #FOAMed #FOAMcc  \n",
      "7317  Man selling WILD MORELS at Ambleside Farmr Mart.Sun.-MUSHROOM forageSECRET IS TO KNOW WHAT TREES they grow under &amp; BEST AFTER FOREST FIRES  \n",
      "2546                                                                 Tell him Rebahe's going to destroy himself @Zenande_Mcfen @NDzedze #Ashes2Ashes  \n",
      "1190    Leicester_Merc : ICYMI - #Ashes 2015: Australia collapse at Trent Bridge - how Twitter reaÛ_ http://t.co/HqeWMREysO) http://t.co/y4y8fclJED  \n",
      "4344                              [Latest Post] Bayelsa poll: Tension in Bayelsa as Patience Jonathan plans to hijack APC PDP http://t.co/B2yvLMPepR  \n",
      "2389                                                             Buyout Giants Bid To Derail å£6bn Worldpay IPO ÛÒ SkyåÊNews http://t.co/94GjsKUR0r  \n",
      "2503                                 (  the abomination that maketh desolate: The antichrist desecrates the Jerusalem temple - Dan 9:27 Matt 24:1  )  \n",
      "7092       @abcnews UK scandal of 2009 caused major upheaval to Parliamentary expenses with subsequent sackings and prison. What are we waiting for?  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 200):\n",
    "    print(df_wrong_predictions.query('target==0').sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C-DL-1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C-DL-1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('C-DL-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (3263, 2172)\n"
     ]
    }
   ],
   "source": [
    "X_test = transformer.transform(pd.read_csv('./test_enriched.csv', index_col='id')).todense()\n",
    "print('X_test shape', X_test.shape)\n",
    "\n",
    "Y_test_predict = tf.cast(tf.greater(tf.nn.sigmoid(model(X_test)), .5), tf.int32)\n",
    "\n",
    "df_example = pd.read_csv('./sample_submission.csv')\n",
    "df_example['target'] = Y_test_predict\n",
    "\n",
    "df_example.to_csv('./C-DL-1-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
